{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJlromJmHINJlNqNaH3+Wl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptuv/AI/blob/main/Hugging_Face_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K7eLp_m8_360"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_gen_pipeline = pipeline('text-generation', model='gpt2')\n",
        "prompt = \"Brazil,Morocco straight to LA,New York Vegas to Africa\"\n",
        "text_gen_pipeline(prompt, max_length=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36XyjP5zAIFD",
        "outputId": "06c30d06-298c-47f6-94ab-ea61ccde0a76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Brazil,Morocco straight to LA,New York Vegas to Africa,San Fernando,Seattle Sounders FC to LA,New York City FC to San'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"For the second time in 3 weeks, foreign ministers of India and China, S Jaishankar and Wang Yi respectively, met Thursday\n",
        "agreeing significantly on the need for a “strong guidance” to complete the disengagement process in eastern Ladakh, an issue\n",
        "blocking return of normalcy to bilateral ties.\n",
        "Jaishankar’s remark stating the same in a post on X seemed to improve upon the outcome of their previous meeting in Kazakhstan\n",
        "earlier this month where he called for redoubling efforts to achieve complete disengagement and both leaders agreed prolongation\n",
        "of the border situation was not in the interest of either side. \"\"\"\n",
        "\n",
        "QA = pipeline(\"question-answering\")\n",
        "question = \"Who is the foreign minister of India?\"\n",
        "Ans1 = QA(question=question, context=text)\n",
        "question = \"What is the outcome of the meeting?\"\n",
        "Ans2 = QA(question=question, context=text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuVFktBTBd_V",
        "outputId": "a3f9917c-e668-46af-ee82-ff4ff23d081f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_gen_pipeline = pipeline('text-generation', model='gpt2')\n",
        "prompt = \"First boil the water and add tea powder\"\n",
        "text_gen_pipeline(prompt, max_length=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZDH7j6sB_dk",
        "outputId": "2e3f911c-40cd-462b-abd8-4f05f6cbe3cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'First boil the water and add tea powder, ground cinnamon, to a large mixing bowl. Stir in the ginger, lime juice, thyme, garlic'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}